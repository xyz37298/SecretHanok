import cv2
import mediapipe as mp
import numpy as np
import joblib
import os
import requests
import base64
import time
from flask import Flask, render_template
from flask_cors import CORS
from flask_socketio import SocketIO, emit

app = Flask(__name__)
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*", async_mode="gevent")

mp_hands = mp.solutions.hands
hands = mp_hands.Hands()
mp_drawing = mp.solutions.drawing_utils

# í˜„ì¬ íŒŒì¼(backend.py)ì˜ í´ë” ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ëª¨ë¸ íŒŒì¼ì„ ì ˆëŒ€ ê²½ë¡œë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
model_path = os.path.join(BASE_DIR, "hand_gesture_recognition_model.pkl")
GDRIVE_URL = "https://drive.google.com/uc?export=download&id=1Aq_Q8jRu04al-S2x2ckwW-MWLpT8YXvk"

# ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
if not os.path.exists(model_path):
    response = requests.get(GDRIVE_URL)
    with open(model_path, "wb") as f:
        f.write(response.content)

# ëª¨ë¸ ë¡œë“œ
model = joblib.load(model_path)
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

GESTURE_TEXTS = {
    "gesture_1": "ë‚´ ì† ì•ˆì˜ ëª¨ë˜",
    "gesture_2": "ê°„ì§ˆ ê°„ì§ˆ ì‹ ë‚˜",
    "gesture_3": "ì˜ë¬¸"
}

GESTURE_SUBTITLES = {
    "gesture_1": "ë‚˜ì˜ ì†ì€ ëª¨ë˜ì„± ìŒ“ê¸°ì˜ MASTER! ê¸°ë‹¤ë ¤, ì¼ë£¨ì ¼ì´ì–´ë„ ê´œì°®ì•„. í•œì›€í¼ ë‚´ê°€ í™• ìŸì•„ë²„ë¦´ê±°ì•¼.",
    "gesture_2": "ì•„! ìƒìƒí•´. ì•Œì•Œì´ ì†ê°€ë½ ì‚¬ì´ì— ë°•íŒ ëª¨ë˜ë“¤ì´ ê¹Œì‹¤ê¹Œì‹¤, ìƒìƒí•˜ê³  ê°„ì§€ëŸ½ê³  ì•„í”„ê³  ì™¸ë¡­ê³  ì‹ ë‚˜.",
    "gesture_3": "ê·¼ë° ì•„ë‹ˆë©´ ì–´ë–¡í•˜ì§€? ì—ì´, ê·¸ë˜ë„ ê´œì°®ì•„. ë‚˜ëŠ” ìš°ì£¼ì˜ ë³„ë§Œí¼ ì§€ì„ í‘œì •ì´ ë§ì•„. ê·¸ë˜ë„ ì–´ì©” ìˆ˜ ì—†ì–ì•„. ê·¸ë˜ë„ ì¥ë‚œê¾¸ëŸ¬ê¸°ì²˜ëŸ¼ í•´ë³´ëŠ”ê±°ì•¼. ë‚˜ëŠ” MASTERë‹ˆê¹Œ! ê·¸ë¦¬ê³  ì‚´ì§ ì„œíˆ° ìˆ˜ë ¨ìƒì´ì•¼."
}

gesture_images = {
    "gesture_1": "static/images/gesture1.png",
    "gesture_2": "static/images/gesture2.png",
    "gesture_3": "static/images/gesture3.png"
}

gesture_sounds = {
    "gesture_1": "/static/sounds/gesture1.mp3",
    "gesture_2": "/static/sounds/gesture2.mp3",
    "gesture_3": "/static/sounds/gesture3.mp3",
}

is_playing = False
last_prediction = None #ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶œë ¥ëœ ë™ì‘ ì €ì¥

@app.route("/")
def index():
    return render_template("index.html")

@socketio.on("video_frame")
def handle_frame(data):
    global is_playing, last_prediction
    print("í”„ë ˆì„ ìˆ˜ì‹ ")
    
    frame_data = base64.b64decode(data.split(",")[1])
    print("í”„ë ˆì„ ë°ì´í„° í¬ê¸°:", len(frame_data))
    if len(frame_data) == 0:
        print("í”„ë ˆì„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
        return
    np_arr = np.frombuffer(frame_data, np.uint8)
    frame = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
    print("í”„ë ˆì„ ë””ì½”ë”© ì™„ë£Œ:", frame.shape if frame is not None else "None")
    
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(frame)

    if not results.multi_hand_landmarks:
        print("ì† ì¸ì‹ ì‹¤íŒ¨")
        return

    print("ì† ì¸ì‹ ì„±ê³µ")

    landmarks_list = []
    prediction = "ì•Œ ìˆ˜ ì—†ìŒ"

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            hand_data = []
            for lm in hand_landmarks.landmark:
                hand_data.append({"x": lm.x, "y": lm.y})
            landmarks_list.append(hand_data)

            # âœ… ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì œìŠ¤ì²˜ ì˜ˆì¸¡
            landmark_array = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()
            if len(landmark_array) == model.n_features_in_:  # ëª¨ë¸ ì…ë ¥ í¬ê¸°ì™€ ë§ëŠ”ì§€ í™•ì¸
                prediction = model.predict([landmark_array])[0]

    # ë§ˆì»¤(ì† ìœ„ì¹˜)ëŠ” í•­ìƒ ì—…ë°ì´íŠ¸
    emit("hand_landmarks", {"landmarks":landmarks_list})


    sound_url = gesture_sounds.get(prediction, "")

    #ê°™ì€ ì œìŠ¤ì³ê°€ ë°˜ë³µë˜ë©´ ì´ë¯¸ì§€/í…ìŠ¤íŠ¸/ìŒì„± ê°±ì‹  x
    if not is_playing and prediction != last_prediction and prediction in GESTURE_TEXTS:
        last_prediction = prediction #ë§ˆì§€ë§‰ ì¶œë ¥ëœ ë™ì‘ ì €ì¥

        gesture_text = GESTURE_TEXTS.get(prediction, "ì†ì„ ë“¤ì–´ì£¼ì„¸ìš”")
        gesture_subtitle = GESTURE_SUBTITLES.get(prediction, "")
        image_url = gesture_images.get(prediction, "")

        emit("gesture_result", {
            "gesture": gesture_text,
            "subtitle": gesture_subtitle,
            "image_url":image_url,
            "sound": sound_url
        })

        # ì¼ì • ì‹œê°„ í›„ ìŒì„±ì´ ëë‚¬ë‹¤ê³  ê°€ì •í•˜ê³  ìƒíƒœ ì´ˆê¸°í™”
        socketio.sleep(2)  # ğŸ’¡ ìŒì„± ê¸¸ì´ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥
        is_playing = False  # ìŒì„±ì´ ëë‚˜ë©´ ë‹¤ì‹œ ì—…ë°ì´íŠ¸ í—ˆìš©

if __name__ == "__main__":
    socketio.run(app, debug=True)
